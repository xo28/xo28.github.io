---
title:          "Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens"
date:           2024-11-26 00:01:00 EST
selected:       true
pub:            "Arxiv"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2024"

# abstract: >-
#   Photo by Pineapple Supply Co. on Unsplash. Please put a tldr (too-long-didnt-read, 1~2 sentences) of your publication here. It is not recommended to put the actual abstract here because it is usually too long to fit in. $\LaTeX$ is supported. $a=b+c$.
cover:          /assets/images/covers/scalinglaw.png
authors:
  - Xu Ouyang
  - Tao Ge
  - Thomas Hartvigsen 
  - Zhisong Zhang 
  - Haitao Mi 
  - Dong Yu
links:
  Paper: https://arxiv.org/pdf/2411.17691
  Quantized checkpoints: https://huggingface.co/Xu-Ouyang
---
